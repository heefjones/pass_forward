{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T22:10:57.234715Z",
     "start_time": "2023-12-08T22:10:51.764669Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper.py\n",
    "from helper import *\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# set up GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T22:10:57.296713Z",
     "start_time": "2023-12-08T22:10:57.236713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1099, 66), 237)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv('./data/clean/features.csv')\n",
    "\n",
    "# drop all aggregated cols\n",
    "df = df.drop(columns=[col for col in df.columns if 'mean' in col or 'std' in col])\n",
    "\n",
    "# check\n",
    "df.shape, len(set(df.player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_player_histories(df):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_pad: FloatTensor, shape (n_players, max_seasons, n_features).\n",
    "      y:      FloatTensor, shape (n_players,).\n",
    "      lengths: IntTensor, shape (n_players,).\n",
    "      mask:   BoolTensor, shape (n_players, max_seasons).\n",
    "      players: list of player names.\n",
    "    \"\"\"\n",
    "\n",
    "    # non-feature columns\n",
    "    non_feat_cols = ['player', 'team_name', 'year', 'target']\n",
    "\n",
    "    # init lists\n",
    "    sequences, labels, players = [], [], []\n",
    "\n",
    "    # iterate through each player\n",
    "    for player, g in df.groupby('player'):\n",
    "        # sort\n",
    "        g = g.sort_values('year').reset_index(drop=True)\n",
    "\n",
    "        # cache the feature matrix once\n",
    "        feat_mat = g.drop(columns=non_feat_cols).values\n",
    "\n",
    "        # iterate through each season\n",
    "        for i in range(len(g)):\n",
    "            # seasons 0 through i\n",
    "            seq = torch.tensor(feat_mat[:i+1], dtype=torch.float32)\n",
    "            \n",
    "            # target for season i\n",
    "            lbl = torch.tensor(g['target'].iloc[i], dtype=torch.float32)\n",
    "\n",
    "            # append to lists\n",
    "            sequences.append(seq)\n",
    "            labels.append(lbl)\n",
    "            players.append(player)\n",
    "\n",
    "    # pad to longest sequence\n",
    "    X_pad = pad_sequence(sequences, batch_first=True)  \n",
    "\n",
    "    # build mask so model knows which timesteps are real\n",
    "    lengths = torch.tensor([seq.size(0) for seq in sequences])\n",
    "    max_len = X_pad.size(1)\n",
    "    mask = torch.arange(max_len)[None, :] < lengths[:, None]\n",
    "\n",
    "    # create labels\n",
    "    y = torch.stack(labels)\n",
    "\n",
    "    return X_pad, y, lengths,  mask, players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# non-feature columns\n",
    "non_feat_cols = ['player', 'team_name', 'year', 'target']\n",
    "feat_cols = [c for c in df.columns if c not in non_feat_cols]\n",
    "\n",
    "# split df into train/val by player to avoid leakage\n",
    "players = df['player'].unique()\n",
    "train_players, val_players = train_test_split(players, test_size=0.2, random_state=SEED)\n",
    "df_train = df[df['player'].isin(train_players)].copy()\n",
    "df_val   = df[df['player'].isin(val_players)].copy()\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "df_train[feat_cols] = scaler.fit_transform(df_train[feat_cols])\n",
    "df_val  [feat_cols] = scaler.transform(df_val[feat_cols])\n",
    "\n",
    "# recombine so our create_player_histories sees scaled data\n",
    "df_scaled = pd.concat([df_train, df_val], ignore_index=True)\n",
    "\n",
    "# build sequences\n",
    "X_pad, y, lengths, mask, players = create_player_histories(df_scaled)\n",
    "\n",
    "# split train/val\n",
    "idx = torch.arange(len(y))\n",
    "train_idx = idx[[p in train_players for p in players]]\n",
    "val_idx = idx[[p in val_players for p in players]]\n",
    "X_train, X_val = X_pad[train_idx], X_pad[val_idx]\n",
    "len_train, len_val = lengths[train_idx], lengths[val_idx]\n",
    "y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "# torch dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, lengths, y):\n",
    "        self.X, self.lengths, self.y = X, lengths, y\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.lengths[i], self.y[i]\n",
    "\n",
    "# create dataset and dataloader\n",
    "train_ds = SeqDataset(X_train, len_train, y_train)\n",
    "val_ds = SeqDataset(X_val, len_val, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "# simple LSTM\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hidden_dim, batch_first=True)\n",
    "        self.head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed, _ = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        out = hn[-1]                    \n",
    "        return self.head(out).squeeze(1)\n",
    "\n",
    "# instantiate\n",
    "model = LSTMRegressor(in_dim=X_pad.size(2), hidden_dim=64).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m yb \u001b[38;5;241m=\u001b[39m yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 9\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m loss  \u001b[38;5;241m=\u001b[39m criterion(preds, yb)\n\u001b[0;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\heefj\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[55], line 60\u001b[0m, in \u001b[0;36mLSTMRegressor.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, lengths):\n\u001b[1;32m---> 60\u001b[0m     packed, _ \u001b[38;5;241m=\u001b[39m pack_padded_sequence(x, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m     _, (hn, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(packed)\n\u001b[0;32m     62\u001b[0m     out \u001b[38;5;241m=\u001b[39m hn[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]                    \n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for Xb, lb, yb in train_dl:\n",
    "        Xb = Xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        preds = model(Xb, lb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()*Xb.size(0)\n",
    "    train_loss /= len(train_ds)\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, lb, yb in val_dl:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            vp = model(Xb, lb)\n",
    "            val_preds.append(vp.cpu())\n",
    "            val_trues.append(yb.cpu())\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_trues = torch.cat(val_trues).numpy()\n",
    "\n",
    "    rmse = mean_squared_error(val_trues, val_preds, squared=False)\n",
    "    r2   = r2_score(val_trues, val_preds)\n",
    "    print(f\"Epoch {epoch+1} → train MSE {train_loss:.4f}, val RMSE {rmse:.4f}, R² {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:19:00.483805Z",
     "start_time": "2023-11-27T21:19:00.470793Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective_function(hidden_dim, num_layers, dropout, batch_size):\n",
    "    # cast continuous values to int\n",
    "    hidden_dim = int(hidden_dim)\n",
    "    num_layers = int(num_layers)\n",
    "    batch_size = int(batch_size)\n",
    "    \n",
    "    # create sequences\n",
    "    X, y = create_seq(feature_subset=all_feats, seq_len=2, df=df)\n",
    "    \n",
    "    # create dataloaders\n",
    "    train_loader, val_loader = create_loaders(X, y, test_size=0.1, batch_size=batch_size)\n",
    "\n",
    "    # create model\n",
    "    model = RNN(input_dim=66, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).to(device)\n",
    "        \n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # train model\n",
    "    train_rmse, train_r2, val_rmse, val_r2, num_epochs, patience = train_val(model, optimizer, train_loader, val_loader)\n",
    "    \n",
    "    # return the negative of the validation metric (since Bayesian optimization minimizes)\n",
    "    return -val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:19:27.727611Z",
     "start_time": "2023-11-27T21:19:27.722607Z"
    }
   },
   "outputs": [],
   "source": [
    "# define bounds of each hyperparameter\n",
    "pbounds = {\n",
    "    'hidden_dim': (4, 1025),\n",
    "    'num_layers': (2, 17),\n",
    "    'dropout': (0, 0.9),\n",
    "    'batch_size': (1, 129)\n",
    "}\n",
    "\n",
    "# create the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,\n",
    "    pbounds=pbounds,\n",
    "    random_state=random_state,\n",
    "    allow_duplicate_points=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T22:04:57.225801Z",
     "start_time": "2023-11-27T21:19:28.927301Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |  dropout  | hidden... | num_la... |\n",
      "-------------------------------------------------------------------------\n",
      "Early stopping on Epoch 20.\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m2.328    \u001b[0m | \u001b[0m0.4517   \u001b[0m | \u001b[0m510.2    \u001b[0m | \u001b[0m4.007    \u001b[0m |\n",
      "Early stopping on Epoch 93.\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m-12.43   \u001b[0m | \u001b[95m19.19    \u001b[0m | \u001b[95m0.1967   \u001b[0m | \u001b[95m431.3    \u001b[0m | \u001b[95m5.722    \u001b[0m |\n",
      "Early stopping on Epoch 54.\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-13.34   \u001b[0m | \u001b[0m11.76    \u001b[0m | \u001b[0m0.3109   \u001b[0m | \u001b[0m174.3    \u001b[0m | \u001b[0m15.18    \u001b[0m |\n",
      "Early stopping on Epoch 21.\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-72.67   \u001b[0m | \u001b[0m122.7    \u001b[0m | \u001b[0m0.03487  \u001b[0m | \u001b[0m717.8    \u001b[0m | \u001b[0m10.59    \u001b[0m |\n",
      "Early stopping on Epoch 73.\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m115.9    \u001b[0m | \u001b[0m0.6002   \u001b[0m | \u001b[0m563.3    \u001b[0m | \u001b[0m12.54    \u001b[0m |\n",
      "Early stopping on Epoch 39.\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-14.19   \u001b[0m | \u001b[0m50.47    \u001b[0m | \u001b[0m0.625    \u001b[0m | \u001b[0m846.2    \u001b[0m | \u001b[0m8.985    \u001b[0m |\n",
      "Early stopping on Epoch 151.\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m-12.06   \u001b[0m | \u001b[95m121.4    \u001b[0m | \u001b[95m0.7234   \u001b[0m | \u001b[95m1.008e+03\u001b[0m | \u001b[95m4.545    \u001b[0m |\n",
      "Early stopping on Epoch 28.\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-13.3    \u001b[0m | \u001b[0m67.7     \u001b[0m | \u001b[0m0.851    \u001b[0m | \u001b[0m667.2    \u001b[0m | \u001b[0m14.91    \u001b[0m |\n",
      "Early stopping on Epoch 88.\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-67.23   \u001b[0m | \u001b[0m125.3    \u001b[0m | \u001b[0m0.167    \u001b[0m | \u001b[0m252.2    \u001b[0m | \u001b[0m15.1     \u001b[0m |\n",
      "Early stopping on Epoch 111.\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-12.7    \u001b[0m | \u001b[0m117.0    \u001b[0m | \u001b[0m0.6825   \u001b[0m | \u001b[0m358.2    \u001b[0m | \u001b[0m8.953    \u001b[0m |\n",
      "Early stopping on Epoch 92.\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-12.25   \u001b[0m | \u001b[0m123.1    \u001b[0m | \u001b[0m0.549    \u001b[0m | \u001b[0m1.008e+03\u001b[0m | \u001b[0m4.959    \u001b[0m |\n",
      "Early stopping on Epoch 34.\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-13.27   \u001b[0m | \u001b[0m47.01    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m603.3    \u001b[0m | \u001b[0m13.86    \u001b[0m |\n",
      "Early stopping on Epoch 60.\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m100.3    \u001b[0m | \u001b[0m0.8896   \u001b[0m | \u001b[0m440.0    \u001b[0m | \u001b[0m7.349    \u001b[0m |\n",
      "Early stopping on Epoch 23.\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-13.64   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m666.5    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 26.\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-68.3    \u001b[0m | \u001b[0m97.43    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m925.0    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 16.\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-66.81   \u001b[0m | \u001b[0m69.34    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m505.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 103.\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-12.45   \u001b[0m | \u001b[0m77.91    \u001b[0m | \u001b[0m0.617    \u001b[0m | \u001b[0m396.5    \u001b[0m | \u001b[0m6.374    \u001b[0m |\n",
      "Early stopping on Epoch 48.\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-13.66   \u001b[0m | \u001b[0m98.42    \u001b[0m | \u001b[0m0.6992   \u001b[0m | \u001b[0m612.6    \u001b[0m | \u001b[0m13.04    \u001b[0m |\n",
      "Early stopping on Epoch 24.\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-13.34   \u001b[0m | \u001b[0m19.27    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m374.0    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 17.\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-54.74   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m405.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 56.\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m58.49    \u001b[0m | \u001b[0m0.2331   \u001b[0m | \u001b[0m429.3    \u001b[0m | \u001b[0m5.409    \u001b[0m |\n",
      "Early stopping on Epoch 72.\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m68.37    \u001b[0m | \u001b[0m0.6358   \u001b[0m | \u001b[0m349.2    \u001b[0m | \u001b[0m8.095    \u001b[0m |\n",
      "Early stopping on Epoch 31.\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m33.82    \u001b[0m | \u001b[0m0.2587   \u001b[0m | \u001b[0m644.8    \u001b[0m | \u001b[0m4.796    \u001b[0m |\n",
      "Early stopping on Epoch 20.\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-12.45   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m571.8    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 30.\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-12.58   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m817.1    \u001b[0m | \u001b[0m4.584    \u001b[0m |\n",
      "Early stopping on Epoch 39.\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m753.7    \u001b[0m | \u001b[0m11.76    \u001b[0m |\n",
      "Early stopping on Epoch 92.\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-12.29   \u001b[0m | \u001b[0m45.18    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m790.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 41.\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-13.27   \u001b[0m | \u001b[0m3.283    \u001b[0m | \u001b[0m0.2424   \u001b[0m | \u001b[0m114.6    \u001b[0m | \u001b[0m15.37    \u001b[0m |\n",
      "Early stopping on Epoch 136.\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-12.3    \u001b[0m | \u001b[0m57.9     \u001b[0m | \u001b[0m0.7927   \u001b[0m | \u001b[0m132.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 184.\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-13.35   \u001b[0m | \u001b[0m51.16    \u001b[0m | \u001b[0m0.1415   \u001b[0m | \u001b[0m72.15    \u001b[0m | \u001b[0m13.14    \u001b[0m |\n",
      "Early stopping on Epoch 306.\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m109.0    \u001b[0m | \u001b[0m0.2251   \u001b[0m | \u001b[0m94.96    \u001b[0m | \u001b[0m8.207    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-13.53   \u001b[0m | \u001b[0m102.7    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m30.44    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-24.73   \u001b[0m | \u001b[0m40.97    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m5.484    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 22.\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m58.5     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.025e+03\u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 28.\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m-12.75   \u001b[0m | \u001b[0m3.373    \u001b[0m | \u001b[0m0.7101   \u001b[0m | \u001b[0m310.8    \u001b[0m | \u001b[0m4.648    \u001b[0m |\n",
      "Early stopping on Epoch 24.\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m-12.15   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m244.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 19.\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m-12.64   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.025e+03\u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 24.\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m-12.52   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m969.0    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 37.\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m-12.57   \u001b[0m | \u001b[0m2.142    \u001b[0m | \u001b[0m0.6756   \u001b[0m | \u001b[0m880.5    \u001b[0m | \u001b[0m3.722    \u001b[0m |\n",
      "Early stopping on Epoch 59.\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m-12.27   \u001b[0m | \u001b[0m26.68    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m707.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 213.\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m145.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 34.\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m58.43    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 93.\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m65.49    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m180.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 72.\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m-38.41   \u001b[0m | \u001b[0m36.97    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m277.9    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 37.\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m114.8    \u001b[0m | \u001b[0m0.6165   \u001b[0m | \u001b[0m824.2    \u001b[0m | \u001b[0m3.182    \u001b[0m |\n",
      "Early stopping on Epoch 77.\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m-45.74   \u001b[0m | \u001b[0m2.21     \u001b[0m | \u001b[0m0.3225   \u001b[0m | \u001b[0m616.5    \u001b[0m | \u001b[0m10.55    \u001b[0m |\n",
      "Early stopping on Epoch 19.\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m30.4     \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m996.4    \u001b[0m | \u001b[0m16.01    \u001b[0m |\n",
      "Early stopping on Epoch 18.\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m-64.31   \u001b[0m | \u001b[0m4.678    \u001b[0m | \u001b[0m0.3943   \u001b[0m | \u001b[0m925.7    \u001b[0m | \u001b[0m11.36    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m-45.7    \u001b[0m | \u001b[0m14.09    \u001b[0m | \u001b[0m0.08807  \u001b[0m | \u001b[0m851.7    \u001b[0m | \u001b[0m14.96    \u001b[0m |\n",
      "Early stopping on Epoch 40.\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m-12.96   \u001b[0m | \u001b[0m2.896    \u001b[0m | \u001b[0m0.3982   \u001b[0m | \u001b[0m994.6    \u001b[0m | \u001b[0m3.549    \u001b[0m |\n",
      "Early stopping on Epoch 53.\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m-13.38   \u001b[0m | \u001b[0m33.73    \u001b[0m | \u001b[0m0.5526   \u001b[0m | \u001b[0m676.7    \u001b[0m | \u001b[0m10.22    \u001b[0m |\n",
      "Early stopping on Epoch 30.\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m-12.35   \u001b[0m | \u001b[0m11.18    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m787.0    \u001b[0m | \u001b[0m2.31     \u001b[0m |\n",
      "Early stopping on Epoch 69.\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m-12.56   \u001b[0m | \u001b[0m42.27    \u001b[0m | \u001b[0m0.3467   \u001b[0m | \u001b[0m397.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 61.\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m-12.65   \u001b[0m | \u001b[0m68.02    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m631.7    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 56.\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m-12.55   \u001b[0m | \u001b[0m74.81    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m819.8    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping on Epoch 208.\n",
      "| \u001b[0m56       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m90.87    \u001b[0m | \u001b[0m0.2738   \u001b[0m | \u001b[0m134.4    \u001b[0m | \u001b[0m16.37    \u001b[0m |\n",
      "Early stopping on Epoch 63.\n",
      "| \u001b[0m57       \u001b[0m | \u001b[0m-12.27   \u001b[0m | \u001b[0m26.34    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m209.5    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 32.\n",
      "| \u001b[0m58       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m41.07    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m749.1    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 75.\n",
      "| \u001b[0m59       \u001b[0m | \u001b[0m-12.38   \u001b[0m | \u001b[0m76.71    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m581.6    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 469.\n",
      "| \u001b[0m60       \u001b[0m | \u001b[0m-13.34   \u001b[0m | \u001b[0m128.5    \u001b[0m | \u001b[0m0.8691   \u001b[0m | \u001b[0m60.56    \u001b[0m | \u001b[0m13.48    \u001b[0m |\n",
      "Early stopping on Epoch 42.\n",
      "| \u001b[0m61       \u001b[0m | \u001b[0m-12.95   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m346.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 16.\n",
      "| \u001b[0m62       \u001b[0m | \u001b[0m-67.17   \u001b[0m | \u001b[0m88.12    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m851.2    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 62.\n",
      "| \u001b[0m63       \u001b[0m | \u001b[0m-12.23   \u001b[0m | \u001b[0m92.22    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m791.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 354.\n",
      "| \u001b[0m64       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m88.78    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m63.16    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 216.\n",
      "| \u001b[0m65       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m70.41    \u001b[0m | \u001b[0m0.4259   \u001b[0m | \u001b[0m101.4    \u001b[0m | \u001b[0m14.26    \u001b[0m |\n",
      "Early stopping on Epoch 117.\n",
      "| \u001b[95m66       \u001b[0m | \u001b[95m-11.87   \u001b[0m | \u001b[95m32.04    \u001b[0m | \u001b[95m0.9      \u001b[0m | \u001b[95m100.7    \u001b[0m | \u001b[95m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 39.\n",
      "| \u001b[0m67       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m796.9    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 66.\n",
      "| \u001b[0m68       \u001b[0m | \u001b[0m-13.38   \u001b[0m | \u001b[0m29.15    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m142.1    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 46.\n",
      "| \u001b[0m69       \u001b[0m | \u001b[0m-12.69   \u001b[0m | \u001b[0m1.455    \u001b[0m | \u001b[0m0.4229   \u001b[0m | \u001b[0m468.5    \u001b[0m | \u001b[0m7.621    \u001b[0m |\n",
      "Early stopping on Epoch 109.\n",
      "| \u001b[0m70       \u001b[0m | \u001b[0m-11.95   \u001b[0m | \u001b[0m103.7    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m326.4    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m71       \u001b[0m | \u001b[0m-52.23   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 142.\n",
      "| \u001b[0m72       \u001b[0m | \u001b[0m-13.74   \u001b[0m | \u001b[0m103.4    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m172.5    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 58.\n",
      "| \u001b[0m73       \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m596.5    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 45.\n",
      "| \u001b[0m74       \u001b[0m | \u001b[0m-13.52   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m709.9    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 27.\n",
      "| \u001b[0m75       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m89.37    \u001b[0m | \u001b[0m0.4598   \u001b[0m | \u001b[0m1.024e+03\u001b[0m | \u001b[0m16.53    \u001b[0m |\n",
      "Early stopping on Epoch 34.\n",
      "| \u001b[0m76       \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m543.5    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 69.\n",
      "| \u001b[0m77       \u001b[0m | \u001b[0m-12.32   \u001b[0m | \u001b[0m36.52    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m564.4    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 19.\n",
      "| \u001b[0m78       \u001b[0m | \u001b[0m-54.83   \u001b[0m | \u001b[0m127.5    \u001b[0m | \u001b[0m0.1224   \u001b[0m | \u001b[0m637.5    \u001b[0m | \u001b[0m11.96    \u001b[0m |\n",
      "Early stopping on Epoch 35.\n",
      "| \u001b[0m79       \u001b[0m | \u001b[0m-13.43   \u001b[0m | \u001b[0m1.461    \u001b[0m | \u001b[0m0.6686   \u001b[0m | \u001b[0m207.6    \u001b[0m | \u001b[0m16.02    \u001b[0m |\n",
      "Early stopping on Epoch 23.\n",
      "| \u001b[0m80       \u001b[0m | \u001b[0m-12.06   \u001b[0m | \u001b[0m2.213    \u001b[0m | \u001b[0m0.801    \u001b[0m | \u001b[0m403.5    \u001b[0m | \u001b[0m2.498    \u001b[0m |\n",
      "Early stopping on Epoch 24.\n",
      "| \u001b[0m81       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m71.91    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m989.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 56.\n",
      "| \u001b[0m82       \u001b[0m | \u001b[0m-12.21   \u001b[0m | \u001b[0m34.63    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m335.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 61.\n",
      "| \u001b[0m83       \u001b[0m | \u001b[0m-13.4    \u001b[0m | \u001b[0m1.515    \u001b[0m | \u001b[0m0.5957   \u001b[0m | \u001b[0m13.61    \u001b[0m | \u001b[0m10.15    \u001b[0m |\n",
      "Early stopping on Epoch 23.\n",
      "| \u001b[0m84       \u001b[0m | \u001b[0m-13.97   \u001b[0m | \u001b[0m44.14    \u001b[0m | \u001b[0m0.0538   \u001b[0m | \u001b[0m820.7    \u001b[0m | \u001b[0m2.642    \u001b[0m |\n",
      "Early stopping on Epoch 70.\n",
      "| \u001b[0m85       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m128.4    \u001b[0m | \u001b[0m0.2497   \u001b[0m | \u001b[0m474.0    \u001b[0m | \u001b[0m5.619    \u001b[0m |\n",
      "Early stopping on Epoch 474.\n",
      "| \u001b[0m86       \u001b[0m | \u001b[0m-12.26   \u001b[0m | \u001b[0m68.58    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m39.56    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 79.\n",
      "| \u001b[0m87       \u001b[0m | \u001b[0m-13.34   \u001b[0m | \u001b[0m126.6    \u001b[0m | \u001b[0m0.2487   \u001b[0m | \u001b[0m524.0    \u001b[0m | \u001b[0m3.615    \u001b[0m |\n",
      "Early stopping on Epoch 103.\n",
      "| \u001b[0m88       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m128.8    \u001b[0m | \u001b[0m0.8372   \u001b[0m | \u001b[0m326.2    \u001b[0m | \u001b[0m16.55    \u001b[0m |\n",
      "Early stopping on Epoch 95.\n",
      "| \u001b[0m89       \u001b[0m | \u001b[0m-12.25   \u001b[0m | \u001b[0m65.35    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m214.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 152.\n",
      "| \u001b[0m90       \u001b[0m | \u001b[0m-12.28   \u001b[0m | \u001b[0m25.4     \u001b[0m | \u001b[0m0.6597   \u001b[0m | \u001b[0m43.94    \u001b[0m | \u001b[0m2.874    \u001b[0m |\n",
      "Early stopping on Epoch 35.\n",
      "| \u001b[0m91       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m55.55    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m710.6    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 89.\n",
      "| \u001b[0m92       \u001b[0m | \u001b[0m-12.54   \u001b[0m | \u001b[0m43.78    \u001b[0m | \u001b[0m0.6941   \u001b[0m | \u001b[0m173.0    \u001b[0m | \u001b[0m2.045    \u001b[0m |\n",
      "Early stopping on Epoch 70.\n",
      "| \u001b[0m93       \u001b[0m | \u001b[0m-13.19   \u001b[0m | \u001b[0m1.29     \u001b[0m | \u001b[0m0.7099   \u001b[0m | \u001b[0m148.2    \u001b[0m | \u001b[0m5.69     \u001b[0m |\n",
      "Early stopping on Epoch 16.\n",
      "| \u001b[0m94       \u001b[0m | \u001b[0m-68.61   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m979.8    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 165.\n",
      "| \u001b[0m95       \u001b[0m | \u001b[0m-13.46   \u001b[0m | \u001b[0m128.2    \u001b[0m | \u001b[0m0.1427   \u001b[0m | \u001b[0m121.0    \u001b[0m | \u001b[0m2.141    \u001b[0m |\n",
      "Early stopping on Epoch 30.\n",
      "| \u001b[0m96       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m113.4    \u001b[0m | \u001b[0m0.03172  \u001b[0m | \u001b[0m1.023e+03\u001b[0m | \u001b[0m3.623    \u001b[0m |\n",
      "Early stopping on Epoch 31.\n",
      "| \u001b[0m97       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m71.95    \u001b[0m | \u001b[0m0.5127   \u001b[0m | \u001b[0m766.7    \u001b[0m | \u001b[0m15.89    \u001b[0m |\n",
      "Early stopping on Epoch 57.\n",
      "| \u001b[0m98       \u001b[0m | \u001b[0m-13.3    \u001b[0m | \u001b[0m56.04    \u001b[0m | \u001b[0m0.8615   \u001b[0m | \u001b[0m376.4    \u001b[0m | \u001b[0m16.12    \u001b[0m |\n",
      "Early stopping on Epoch 50.\n",
      "| \u001b[0m99       \u001b[0m | \u001b[0m-12.97   \u001b[0m | \u001b[0m7.878    \u001b[0m | \u001b[0m0.03297  \u001b[0m | \u001b[0m80.96    \u001b[0m | \u001b[0m2.556    \u001b[0m |\n",
      "Early stopping on Epoch 63.\n",
      "| \u001b[0m100      \u001b[0m | \u001b[0m-13.3    \u001b[0m | \u001b[0m74.73    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m317.0    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 40.\n",
      "| \u001b[0m101      \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m66.94    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m798.5    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 54.\n",
      "| \u001b[0m102      \u001b[0m | \u001b[0m-13.29   \u001b[0m | \u001b[0m95.7     \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m345.9    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 16.\n",
      "| \u001b[0m103      \u001b[0m | \u001b[0m-68.01   \u001b[0m | \u001b[0m95.91    \u001b[0m | \u001b[0m0.204    \u001b[0m | \u001b[0m586.5    \u001b[0m | \u001b[0m16.5     \u001b[0m |\n",
      "Early stopping on Epoch 56.\n",
      "| \u001b[0m104      \u001b[0m | \u001b[0m-12.0    \u001b[0m | \u001b[0m54.82    \u001b[0m | \u001b[0m0.5932   \u001b[0m | \u001b[0m582.8    \u001b[0m | \u001b[0m5.176    \u001b[0m |\n",
      "Early stopping on Epoch 55.\n",
      "| \u001b[0m105      \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m128.4    \u001b[0m | \u001b[0m0.6564   \u001b[0m | \u001b[0m550.8    \u001b[0m | \u001b[0m2.18     \u001b[0m |\n",
      "Early stopping on Epoch 143.\n",
      "| \u001b[0m106      \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m71.72    \u001b[0m | \u001b[0m0.5931   \u001b[0m | \u001b[0m158.6    \u001b[0m | \u001b[0m3.285    \u001b[0m |\n",
      "Early stopping on Epoch 66.\n",
      "| \u001b[0m107      \u001b[0m | \u001b[0m-16.89   \u001b[0m | \u001b[0m30.94    \u001b[0m | \u001b[0m0.1704   \u001b[0m | \u001b[0m1.024e+03\u001b[0m | \u001b[0m7.453    \u001b[0m |\n",
      "Early stopping on Epoch 124.\n",
      "| \u001b[0m108      \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m44.41    \u001b[0m | \u001b[0m0.4511   \u001b[0m | \u001b[0m202.6    \u001b[0m | \u001b[0m16.06    \u001b[0m |\n",
      "Early stopping on Epoch 88.\n",
      "| \u001b[0m109      \u001b[0m | \u001b[0m-12.57   \u001b[0m | \u001b[0m66.12    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m558.5    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 30.\n",
      "| \u001b[0m110      \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m31.94    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m456.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "# iterate through feature space\n",
    "optimizer.maximize(init_points=10, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T22:05:13.703234Z",
     "start_time": "2023-11-27T22:05:13.683217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32.03638856422199,\n",
       " 'dropout': 0.9,\n",
       " 'hidden_dim': 100.74795411866992,\n",
       " 'num_layers': 2.0}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at params that gave lowest validation RMSE\n",
    "best_hyperparams = optimizer.max['params']\n",
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best val_rmse = __11.87__ with a sequence length of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023 predictions\n",
    "- This model will be trained on the entire train/val data, and then will predict 2023 offensive grade from the 2022 holdout set.\n",
    "- Since we are using a sequence length of 2 seasons to predict the third, we will use 2021 & 2022 seasons (to predict 2022 target) as the test set.\n",
    "- This means that players with under 3 seasons played can't be predicted on. I will use the best model (Random Forest) from [this notebook](./models_1.ipynb) to make these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:40:04.861932Z",
     "start_time": "2023-11-28T01:40:04.812887Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the names of the 48 players that have 2023 targets\n",
    "player_names_2023 = players_2022.player.values\n",
    "\n",
    "# master_df includes 2022 rows. create a subset for these players, get players with at least 2 seasons\n",
    "players_subset = master_df[master_df['player'].isin(player_names_2023)]\n",
    "players_subset = players_subset.groupby('player').filter(lambda x: len(x) >= 2)\n",
    "\n",
    "# get last two rows for each player\n",
    "seq_test = players_subset.groupby('player').apply(lambda x: x.tail(2)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:40:08.135974Z",
     "start_time": "2023-11-28T01:40:05.316410Z"
    }
   },
   "outputs": [],
   "source": [
    "# train sequences\n",
    "X_train, y_train = create_seq(feature_subset=all_feats, seq_len=2, df=df)\n",
    "\n",
    "# test sequences\n",
    "X_test, y_test = create_seq(feature_subset=all_feats, seq_len=2, df=seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:40:08.709878Z",
     "start_time": "2023-11-28T01:40:08.692863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((752, 2, 66), (752,), (42, 2, 66), (42,))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 752 total examples to train on, 42 QBs to predict on\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:40:10.729348Z",
     "start_time": "2023-11-28T01:40:10.711331Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_loader = create_loaders(X_train, y_train, test_size=0, batch_size=32)\n",
    "test_loader = create_loaders(X_test, y_test, test_size=0, batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:48:32.736071Z",
     "start_time": "2023-11-28T01:48:32.729064Z"
    }
   },
   "outputs": [],
   "source": [
    "# best sequence model\n",
    "best_seq = RNN(input_dim=len(all_feats), hidden_dim=98, num_layers=2, dropout=0.9).to(device)\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.AdamW(best_seq.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:48:37.369063Z",
     "start_time": "2023-11-28T01:48:32.873199Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 100 epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# training mode\n",
    "best_seq.train()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):  \n",
    "    # train batches\n",
    "    for x, y in train_loader:\n",
    "        # put x and y on gpu\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        train_preds = best_seq(x)\n",
    "        # calc loss\n",
    "        train_loss = criterion(train_preds, y)\n",
    "        # backward pass\n",
    "        train_loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:48:37.385078Z",
     "start_time": "2023-11-28T01:48:37.371066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.895\n",
      "R^2: 0.317\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "best_seq.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for x, y in test_loader:\n",
    "        # put x and y on gpu\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        test_preds = best_seq(x)\n",
    "        # calc loss\n",
    "        test_loss = criterion(test_preds, y)\n",
    "\n",
    "        # performance metrics\n",
    "        rmse = np.sqrt(((test_preds - y) ** 2).sum().item() / y.shape[0])\n",
    "        r2 = r2_score(y.numpy(force=True), test_preds.numpy(force=True))\n",
    "\n",
    "print(f'RMSE: {rmse:.3f}')\n",
    "print(f'R^2: {r2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On the 42/48 QBs who have 3+ seasons played, our model predicts their 2023 offensive grade with an RMSE of 12.895."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:49:47.599269Z",
     "start_time": "2023-11-28T01:49:47.580251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 70)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 2022 players who can't be predicted on with sequence model (players with less than 2 seasons)\n",
    "players_subset = master_df[master_df['player'].isin(player_names_2023)]\n",
    "players_subset = players_subset.groupby('player').filter(lambda x: len(x) < 2)\n",
    "\n",
    "# get last row for each player\n",
    "non_seq_test = players_subset.groupby('player').apply(lambda x: x.tail(1)).reset_index(drop=True)\n",
    "non_seq_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6 QBs in 2023 with less than 3 seasons played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:50:11.353517Z",
     "start_time": "2023-11-28T01:50:09.810113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.658\n",
      "R^2: 0.245\n"
     ]
    }
   ],
   "source": [
    "# best random forest\n",
    "best_rf = RandomForestRegressor(random_state=random_state, min_samples_split=112)\n",
    "\n",
    "# features and target\n",
    "X_train = df[all_feats]\n",
    "y_train = df.target\n",
    "X_test = non_seq_test[all_feats]\n",
    "y_test = non_seq_test.target\n",
    "\n",
    "# create pieline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('rf', best_rf)\n",
    "    ])\n",
    "\n",
    "# train on entire dataset\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "preds = pipeline.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "print(f'RMSE: {rmse:.3f}')\n",
    "print(f'R^2: {r2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:57:16.248421Z",
     "start_time": "2023-11-28T01:57:16.234409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.747\n",
      "R^2: 0.338\n"
     ]
    }
   ],
   "source": [
    "# combine preds from the two models\n",
    "y_pred = np.concatenate([test_preds.squeeze(-1).cpu().numpy(), preds])\n",
    "\n",
    "# get true values\n",
    "y_true = np.concatenate([y.squeeze(-1).cpu().numpy(), y_test])\n",
    "\n",
    "# look at overall performance\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f'RMSE: {rmse:.3f}')\n",
    "print(f'R^2: {r2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using an RNN with a sequence length of 2 (paired with best Random Forest), we achieve a RMSE of __12.75__.\n",
    "- This is worse performance than just the Random Forest from [models_1](./models_1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T02:36:52.739556Z",
     "start_time": "2023-11-28T02:36:52.713532Z"
    }
   },
   "outputs": [],
   "source": [
    "# player names\n",
    "player_names = seq_test.player.unique().tolist() + non_seq_test.player.unique().tolist()\n",
    "\n",
    "# teams\n",
    "team_names = []\n",
    "for _, group in seq_test.groupby('player'):\n",
    "    team_names.append(group.iloc[-1].team_name)\n",
    "\n",
    "team_names.extend(non_seq_test.team_name.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
