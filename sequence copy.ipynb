{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T22:10:57.234715Z",
     "start_time": "2023-12-08T22:10:51.764669Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper.py\n",
    "from helper import *\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# set up GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T22:10:57.296713Z",
     "start_time": "2023-12-08T22:10:57.236713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1099, 66), 237)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv('./data/clean/features.csv')\n",
    "\n",
    "# drop all aggregated cols\n",
    "df = df.drop(columns=[col for col in df.columns if 'mean' in col or 'std' in col])\n",
    "\n",
    "# check\n",
    "df.shape, len(set(df.player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:24:43.364865Z",
     "start_time": "2023-11-27T19:24:43.349851Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_seq(df, seq_len):\n",
    "    '''\n",
    "    Pack data into sequences of seq_len NFL seasons. Return features and target variables.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame) - dataframe to create the sequences from\n",
    "    seq_len (int) - number of seasons to use to predict the target\n",
    "    \n",
    "    Returns:\n",
    "    X (numpy.ndarray) - features\n",
    "    y (numpy.ndarray) - targets\n",
    "    '''\n",
    "    \n",
    "    # get players with at least seq_len seasons\n",
    "    df_grouped = df.groupby('player').filter(lambda x: len(x) >= seq_len)\n",
    "\n",
    "    # init empty lists for sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # non-feature cols\n",
    "    non_feat_cols = ['player', 'team_name', 'year', 'target']\n",
    "\n",
    "    # iterate over each player\n",
    "    for player, group in df_grouped.groupby('player'):\n",
    "        # iterate over the group to create sequences\n",
    "        for i in range((len(group) - seq_len) + 1):\n",
    "            # each sequence is a list of rows/features from two consecutive seasons\n",
    "            sequence = group.iloc[i:(i + seq_len)].drop(non_feat_cols, axis=1).to_dict('records')\n",
    "            \n",
    "            # the label is the 'target' col from the final season in sequence\n",
    "            label = group.iloc[i + seq_len - 1]['target']\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "\n",
    "    # unpack the dicts in each sequence, fill null values with 0\n",
    "    unpacked_sequences = [[[val for val in list(d.values())] for d in sequence] for sequence in sequences]   \n",
    "\n",
    "    # convert sequences and labels into np array\n",
    "    X = np.array(unpacked_sequences)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:24:51.483022Z",
     "start_time": "2023-11-27T19:24:43.600852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sequences of 2 seasons:\n",
      "Shape of X = (862, 2, 62)\n",
      "Shape of y = (862,)\n",
      "\n",
      "Using sequences of 3 seasons:\n",
      "Shape of X = (674, 3, 62)\n",
      "Shape of y = (674,)\n",
      "\n",
      "Using sequences of 4 seasons:\n",
      "Shape of X = (529, 4, 62)\n",
      "Shape of y = (529,)\n",
      "\n",
      "Using sequences of 5 seasons:\n",
      "Shape of X = (411, 5, 62)\n",
      "Shape of y = (411,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_len in [2, 3, 4, 5]:\n",
    "    X, y = create_seq(seq_len=seq_len, df=df)\n",
    "    print(f'Using sequences of {seq_len} seasons:')\n",
    "    print(f'Shape of X = {X.shape}')\n",
    "    print(f'Shape of y = {y.shape}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_player_histories(df):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_pad: FloatTensor, shape (n_players, max_seasons, n_features)\n",
    "      y:      FloatTensor, shape (n_players,)\n",
    "      mask:   BoolTensor, shape (n_players, max_seasons)\n",
    "      players: list of player names, length n_players\n",
    "    \"\"\"\n",
    "\n",
    "    # non-feature columns\n",
    "    non_feat_cols = ['player', 'team_name', 'year', 'target']\n",
    "\n",
    "    # init lists\n",
    "    sequences, labels, players = [], [], []\n",
    "\n",
    "    # iterate through each player\n",
    "    for player, g in df.groupby('player'):\n",
    "        # sort\n",
    "        g = g.sort_values('year').reset_index(drop=True)\n",
    "        \n",
    "        # get length of group\n",
    "        n = len(g)\n",
    "\n",
    "        # cache the feature matrix once\n",
    "        feat_mat = g.drop(columns=non_feat_cols).values\n",
    "\n",
    "        # for each window end i = 0 ... n-1, make a sequence of [0:i] predicting target[i]\n",
    "        for i in range(n):\n",
    "            # seasons 0 through i\n",
    "            seq = torch.tensor(feat_mat[: i+1, :], dtype=torch.float32)\n",
    "            \n",
    "            # target for season i\n",
    "            lbl = torch.tensor(g['target'].iloc[i], dtype=torch.float32)\n",
    "\n",
    "            # append to lists\n",
    "            sequences.append(seq)\n",
    "            labels.append(lbl)\n",
    "            players.append(player)\n",
    "\n",
    "    # pad to longest sequence\n",
    "    X_pad = pad_sequence(sequences, batch_first=True)  \n",
    "\n",
    "    # build mask so model knows which timesteps are real\n",
    "    lengths = torch.tensor([seq.size(0) for seq in sequences])\n",
    "    max_len = X_pad.size(1)\n",
    "    mask = torch.arange(max_len)[None, :] < lengths[:, None]\n",
    "\n",
    "    # create labels\n",
    "    y = torch.stack(labels)\n",
    "\n",
    "    return X_pad, y, mask, players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pad, y, mask, players = create_player_histories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1099, 18, 62])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 62])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:24:51.499036Z",
     "start_time": "2023-11-27T19:24:51.486025Z"
    }
   },
   "outputs": [],
   "source": [
    "# StandardScaler won't accept 3-dimensional data as input, so we create a function\n",
    "def standardize_3d(X_train, X_val=None):\n",
    "    '''\n",
    "    \n",
    "    Reshapes X array from 3D to 2D and standardizes.\n",
    "    \n",
    "    Parameters\n",
    "    X_train (np.array) - X_train array\n",
    "    X_val: (np.array) - X_val array\n",
    "    \n",
    "    Returns\n",
    "    X_train_standardized_3d (np.array) - standardized X_train array in 3-dimensional form\n",
    "    X_val_standardized_3d (np.array) - standardized X_val array in 3-dimensional form\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        # reshape the data to be 2D -> (n_samples * n_time_steps, n_features)\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "        X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "\n",
    "        # create scaler, fit on the training data\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_reshaped)\n",
    "\n",
    "        # transform the data\n",
    "        X_train_standardized_2d = scaler.transform(X_train_reshaped)\n",
    "        X_val_standardized_2d = scaler.transform(X_val_reshaped)\n",
    "\n",
    "        # reshape back to 3D\n",
    "        X_train_standardized_3d = X_train_standardized_2d.reshape(X_train.shape)\n",
    "        X_val_standardized_3d = X_val_standardized_2d.reshape(X_val.shape)\n",
    "\n",
    "        return X_train_standardized_3d, X_val_standardized_3d\n",
    "    \n",
    "    else:\n",
    "        # reshape the data to be 2D -> (n_samples * n_time_steps, n_features)\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "        # create scaler, fit on the training data\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_reshaped)\n",
    "\n",
    "        # transform the data\n",
    "        X_train_standardized_2d = scaler.transform(X_train_reshaped)\n",
    "\n",
    "        # reshape back to 3D\n",
    "        X_train_standardized_3d = X_train_standardized_2d.reshape(X_train.shape)\n",
    "\n",
    "        return X_train_standardized_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T02:32:47.578141Z",
     "start_time": "2023-11-28T02:32:47.569133Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_loaders(X, y, batch_size, test_size, random_state=random_state):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    Split data into train and val sets, standardize data, return training and valing dataloaders.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray) - features\n",
    "    y (numpy.ndarray) - targets\n",
    "    batch_size (int) - batch size to be used in the training dataloader\n",
    "    test_size (float) - percentage of data to be set aside as validation set\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    train_loader (torch.utils.data.DataLoader) - train dataloader\n",
    "    val_loader (torch.utils.data.DataLoader) - val dataloader \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # typecast for compatibility\n",
    "    y = y.astype('float32')\n",
    "    \n",
    "    if test_size > 0:\n",
    "        # train/val split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # standardize the data\n",
    "        X_train_standardized, X_val_standardized = standardize_3d(X_train, X_val)\n",
    "\n",
    "        # convert sequences and labels into tensors\n",
    "        X_train_tensor = torch.tensor(X_train_standardized, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val_standardized, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "        # datasets\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "        # dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=X_val_tensor.shape[0], shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    # single dataloaders for final generalization stage\n",
    "    else:\n",
    "        # standardize the data\n",
    "        X_train_standardized = standardize_3d(X)\n",
    "\n",
    "        # convert sequences and labels into tensors\n",
    "        X_train_tensor = torch.tensor(X_train_standardized, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "        # datasets\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "        # dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:06:37.834348Z",
     "start_time": "2023-11-27T21:06:37.816330Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val(model, optimizer, train_loader, val_loader, patience=15, device=device):\n",
    "    \n",
    "    # loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # 500 epochs\n",
    "    num_epochs = 500\n",
    "\n",
    "    # early stopping\n",
    "    stopped = False\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # training mode\n",
    "        model.train()\n",
    "\n",
    "        # sum of squared error, preds, and y. reset at the start of every epoch\n",
    "        sse = 0\n",
    "        epoch_preds = []\n",
    "        epoch_y = []\n",
    "\n",
    "        # train batches\n",
    "        for x, y in train_loader:\n",
    "            # put x and y on gpu\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            train_preds = model(x)\n",
    "            # calc loss\n",
    "            train_loss = criterion(train_preds, y)\n",
    "            # backward pass\n",
    "            train_loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            # accumulate squared errors\n",
    "            sse += ((train_preds - y) ** 2).sum().item()\n",
    "\n",
    "            # get preds and y from batch to calc r2\n",
    "            epoch_preds.extend(train_preds.numpy(force=True))\n",
    "            epoch_y.extend(y.numpy(force=True))\n",
    "\n",
    "        # train metrics\n",
    "        train_rmse = np.sqrt(sse / len(train_loader.dataset))\n",
    "        train_r2 = r2_score(epoch_y, epoch_preds)\n",
    "\n",
    "\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # val loop (single batch)\n",
    "            for x, y in val_loader:\n",
    "                # put x and y on gpu\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                # forward pass\n",
    "                val_preds = model(x)\n",
    "                # calc loss\n",
    "                val_loss = criterion(val_preds, y)\n",
    "\n",
    "        # val metrics\n",
    "        val_rmse = np.sqrt(((val_preds - y) ** 2).sum().item() / y.shape[0])\n",
    "        val_r2 = r2_score(y.numpy(force=True), val_preds.numpy(force=True))\n",
    "\n",
    "    \n",
    "    \n",
    "        # early stopping\n",
    "        if val_rmse < best_val_loss:\n",
    "            best_val_loss = val_rmse\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "            # break out of train loop if we reach patience value\n",
    "            if epochs_without_improvement == patience:\n",
    "                stopped = True\n",
    "                stopped_epoch = epoch + 1\n",
    "                print(f'Early stopping on Epoch {stopped_epoch}.')\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    # get number of epochs that the model was trained for            \n",
    "    if stopped:\n",
    "        num_epochs = stopped_epoch\n",
    "\n",
    "\n",
    "\n",
    "    return train_rmse, train_r2, val_rmse, val_r2, num_epochs, patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:06:38.652329Z",
     "start_time": "2023-11-27T21:06:38.633311Z"
    }
   },
   "outputs": [],
   "source": [
    "# recurrent neural network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1, dropout=0):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "# long-short-term-memory rnn\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1, dropout=0):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "# gated-recurrent-unit rnn\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1, dropout=0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "- Iterating through parameter space to find best sequence model\n",
    "- I want to find the best model for each specific sequence length. I will perform a round of Bayesian optimization for sequences of 2, 3, and 4 seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:18:56.784554Z",
     "start_time": "2023-11-27T21:18:56.779550Z"
    }
   },
   "outputs": [],
   "source": [
    "# all features\n",
    "all_feats = df.drop(['player', 'team_name', 'year', 'target'], axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:19:00.483805Z",
     "start_time": "2023-11-27T21:19:00.470793Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective_function(hidden_dim, num_layers, dropout, batch_size):\n",
    "    # cast continuous values to int\n",
    "    hidden_dim = int(hidden_dim)\n",
    "    num_layers = int(num_layers)\n",
    "    batch_size = int(batch_size)\n",
    "    \n",
    "    # create sequences\n",
    "    X, y = create_seq(feature_subset=all_feats, seq_len=2, df=df)\n",
    "    \n",
    "    # create dataloaders\n",
    "    train_loader, val_loader = create_loaders(X, y, test_size=0.1, batch_size=batch_size)\n",
    "\n",
    "    # create model\n",
    "    model = RNN(input_dim=66, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).to(device)\n",
    "        \n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # train model\n",
    "    train_rmse, train_r2, val_rmse, val_r2, num_epochs, patience = train_val(model, optimizer, train_loader, val_loader)\n",
    "    \n",
    "    # return the negative of the validation metric (since Bayesian optimization minimizes)\n",
    "    return -val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T21:19:27.727611Z",
     "start_time": "2023-11-27T21:19:27.722607Z"
    }
   },
   "outputs": [],
   "source": [
    "# define bounds of each hyperparameter\n",
    "pbounds = {\n",
    "    'hidden_dim': (4, 1025),\n",
    "    'num_layers': (2, 17),\n",
    "    'dropout': (0, 0.9),\n",
    "    'batch_size': (1, 129)\n",
    "}\n",
    "\n",
    "# create the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,\n",
    "    pbounds=pbounds,\n",
    "    random_state=random_state,\n",
    "    allow_duplicate_points=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T22:04:57.225801Z",
     "start_time": "2023-11-27T21:19:28.927301Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |  dropout  | hidden... | num_la... |\n",
      "-------------------------------------------------------------------------\n",
      "Early stopping on Epoch 20.\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m2.328    \u001b[0m | \u001b[0m0.4517   \u001b[0m | \u001b[0m510.2    \u001b[0m | \u001b[0m4.007    \u001b[0m |\n",
      "Early stopping on Epoch 93.\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m-12.43   \u001b[0m | \u001b[95m19.19    \u001b[0m | \u001b[95m0.1967   \u001b[0m | \u001b[95m431.3    \u001b[0m | \u001b[95m5.722    \u001b[0m |\n",
      "Early stopping on Epoch 54.\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-13.34   \u001b[0m | \u001b[0m11.76    \u001b[0m | \u001b[0m0.3109   \u001b[0m | \u001b[0m174.3    \u001b[0m | \u001b[0m15.18    \u001b[0m |\n",
      "Early stopping on Epoch 21.\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-72.67   \u001b[0m | \u001b[0m122.7    \u001b[0m | \u001b[0m0.03487  \u001b[0m | \u001b[0m717.8    \u001b[0m | \u001b[0m10.59    \u001b[0m |\n",
      "Early stopping on Epoch 73.\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m115.9    \u001b[0m | \u001b[0m0.6002   \u001b[0m | \u001b[0m563.3    \u001b[0m | \u001b[0m12.54    \u001b[0m |\n",
      "Early stopping on Epoch 39.\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-14.19   \u001b[0m | \u001b[0m50.47    \u001b[0m | \u001b[0m0.625    \u001b[0m | \u001b[0m846.2    \u001b[0m | \u001b[0m8.985    \u001b[0m |\n",
      "Early stopping on Epoch 151.\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m-12.06   \u001b[0m | \u001b[95m121.4    \u001b[0m | \u001b[95m0.7234   \u001b[0m | \u001b[95m1.008e+03\u001b[0m | \u001b[95m4.545    \u001b[0m |\n",
      "Early stopping on Epoch 28.\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-13.3    \u001b[0m | \u001b[0m67.7     \u001b[0m | \u001b[0m0.851    \u001b[0m | \u001b[0m667.2    \u001b[0m | \u001b[0m14.91    \u001b[0m |\n",
      "Early stopping on Epoch 88.\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-67.23   \u001b[0m | \u001b[0m125.3    \u001b[0m | \u001b[0m0.167    \u001b[0m | \u001b[0m252.2    \u001b[0m | \u001b[0m15.1     \u001b[0m |\n",
      "Early stopping on Epoch 111.\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-12.7    \u001b[0m | \u001b[0m117.0    \u001b[0m | \u001b[0m0.6825   \u001b[0m | \u001b[0m358.2    \u001b[0m | \u001b[0m8.953    \u001b[0m |\n",
      "Early stopping on Epoch 92.\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-12.25   \u001b[0m | \u001b[0m123.1    \u001b[0m | \u001b[0m0.549    \u001b[0m | \u001b[0m1.008e+03\u001b[0m | \u001b[0m4.959    \u001b[0m |\n",
      "Early stopping on Epoch 34.\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-13.27   \u001b[0m | \u001b[0m47.01    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m603.3    \u001b[0m | \u001b[0m13.86    \u001b[0m |\n",
      "Early stopping on Epoch 60.\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m100.3    \u001b[0m | \u001b[0m0.8896   \u001b[0m | \u001b[0m440.0    \u001b[0m | \u001b[0m7.349    \u001b[0m |\n",
      "Early stopping on Epoch 23.\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-13.64   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m666.5    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 26.\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-68.3    \u001b[0m | \u001b[0m97.43    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m925.0    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 16.\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-66.81   \u001b[0m | \u001b[0m69.34    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m505.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 103.\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-12.45   \u001b[0m | \u001b[0m77.91    \u001b[0m | \u001b[0m0.617    \u001b[0m | \u001b[0m396.5    \u001b[0m | \u001b[0m6.374    \u001b[0m |\n",
      "Early stopping on Epoch 48.\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-13.66   \u001b[0m | \u001b[0m98.42    \u001b[0m | \u001b[0m0.6992   \u001b[0m | \u001b[0m612.6    \u001b[0m | \u001b[0m13.04    \u001b[0m |\n",
      "Early stopping on Epoch 24.\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-13.34   \u001b[0m | \u001b[0m19.27    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m374.0    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 17.\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-54.74   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m405.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 56.\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m58.49    \u001b[0m | \u001b[0m0.2331   \u001b[0m | \u001b[0m429.3    \u001b[0m | \u001b[0m5.409    \u001b[0m |\n",
      "Early stopping on Epoch 72.\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m68.37    \u001b[0m | \u001b[0m0.6358   \u001b[0m | \u001b[0m349.2    \u001b[0m | \u001b[0m8.095    \u001b[0m |\n",
      "Early stopping on Epoch 31.\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m33.82    \u001b[0m | \u001b[0m0.2587   \u001b[0m | \u001b[0m644.8    \u001b[0m | \u001b[0m4.796    \u001b[0m |\n",
      "Early stopping on Epoch 20.\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-12.45   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m571.8    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 30.\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-12.58   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m817.1    \u001b[0m | \u001b[0m4.584    \u001b[0m |\n",
      "Early stopping on Epoch 39.\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m753.7    \u001b[0m | \u001b[0m11.76    \u001b[0m |\n",
      "Early stopping on Epoch 92.\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-12.29   \u001b[0m | \u001b[0m45.18    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m790.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 41.\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-13.27   \u001b[0m | \u001b[0m3.283    \u001b[0m | \u001b[0m0.2424   \u001b[0m | \u001b[0m114.6    \u001b[0m | \u001b[0m15.37    \u001b[0m |\n",
      "Early stopping on Epoch 136.\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-12.3    \u001b[0m | \u001b[0m57.9     \u001b[0m | \u001b[0m0.7927   \u001b[0m | \u001b[0m132.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 184.\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-13.35   \u001b[0m | \u001b[0m51.16    \u001b[0m | \u001b[0m0.1415   \u001b[0m | \u001b[0m72.15    \u001b[0m | \u001b[0m13.14    \u001b[0m |\n",
      "Early stopping on Epoch 306.\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m109.0    \u001b[0m | \u001b[0m0.2251   \u001b[0m | \u001b[0m94.96    \u001b[0m | \u001b[0m8.207    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-13.53   \u001b[0m | \u001b[0m102.7    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m30.44    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-24.73   \u001b[0m | \u001b[0m40.97    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m5.484    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 22.\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m58.5     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m1.025e+03\u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 28.\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m-12.75   \u001b[0m | \u001b[0m3.373    \u001b[0m | \u001b[0m0.7101   \u001b[0m | \u001b[0m310.8    \u001b[0m | \u001b[0m4.648    \u001b[0m |\n",
      "Early stopping on Epoch 24.\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m-12.15   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m244.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 19.\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m-12.64   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.025e+03\u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 24.\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m-12.52   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m969.0    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 37.\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m-12.57   \u001b[0m | \u001b[0m2.142    \u001b[0m | \u001b[0m0.6756   \u001b[0m | \u001b[0m880.5    \u001b[0m | \u001b[0m3.722    \u001b[0m |\n",
      "Early stopping on Epoch 59.\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m-12.27   \u001b[0m | \u001b[0m26.68    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m707.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 213.\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m145.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 34.\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m58.43    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 93.\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m65.49    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m180.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 72.\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m-38.41   \u001b[0m | \u001b[0m36.97    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m277.9    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 37.\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m114.8    \u001b[0m | \u001b[0m0.6165   \u001b[0m | \u001b[0m824.2    \u001b[0m | \u001b[0m3.182    \u001b[0m |\n",
      "Early stopping on Epoch 77.\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m-45.74   \u001b[0m | \u001b[0m2.21     \u001b[0m | \u001b[0m0.3225   \u001b[0m | \u001b[0m616.5    \u001b[0m | \u001b[0m10.55    \u001b[0m |\n",
      "Early stopping on Epoch 19.\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m30.4     \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m996.4    \u001b[0m | \u001b[0m16.01    \u001b[0m |\n",
      "Early stopping on Epoch 18.\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m-64.31   \u001b[0m | \u001b[0m4.678    \u001b[0m | \u001b[0m0.3943   \u001b[0m | \u001b[0m925.7    \u001b[0m | \u001b[0m11.36    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m-45.7    \u001b[0m | \u001b[0m14.09    \u001b[0m | \u001b[0m0.08807  \u001b[0m | \u001b[0m851.7    \u001b[0m | \u001b[0m14.96    \u001b[0m |\n",
      "Early stopping on Epoch 40.\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m-12.96   \u001b[0m | \u001b[0m2.896    \u001b[0m | \u001b[0m0.3982   \u001b[0m | \u001b[0m994.6    \u001b[0m | \u001b[0m3.549    \u001b[0m |\n",
      "Early stopping on Epoch 53.\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m-13.38   \u001b[0m | \u001b[0m33.73    \u001b[0m | \u001b[0m0.5526   \u001b[0m | \u001b[0m676.7    \u001b[0m | \u001b[0m10.22    \u001b[0m |\n",
      "Early stopping on Epoch 30.\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m-12.35   \u001b[0m | \u001b[0m11.18    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m787.0    \u001b[0m | \u001b[0m2.31     \u001b[0m |\n",
      "Early stopping on Epoch 69.\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m-12.56   \u001b[0m | \u001b[0m42.27    \u001b[0m | \u001b[0m0.3467   \u001b[0m | \u001b[0m397.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 61.\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m-12.65   \u001b[0m | \u001b[0m68.02    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m631.7    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 56.\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m-12.55   \u001b[0m | \u001b[0m74.81    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m819.8    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping on Epoch 208.\n",
      "| \u001b[0m56       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m90.87    \u001b[0m | \u001b[0m0.2738   \u001b[0m | \u001b[0m134.4    \u001b[0m | \u001b[0m16.37    \u001b[0m |\n",
      "Early stopping on Epoch 63.\n",
      "| \u001b[0m57       \u001b[0m | \u001b[0m-12.27   \u001b[0m | \u001b[0m26.34    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m209.5    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 32.\n",
      "| \u001b[0m58       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m41.07    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m749.1    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 75.\n",
      "| \u001b[0m59       \u001b[0m | \u001b[0m-12.38   \u001b[0m | \u001b[0m76.71    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m581.6    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 469.\n",
      "| \u001b[0m60       \u001b[0m | \u001b[0m-13.34   \u001b[0m | \u001b[0m128.5    \u001b[0m | \u001b[0m0.8691   \u001b[0m | \u001b[0m60.56    \u001b[0m | \u001b[0m13.48    \u001b[0m |\n",
      "Early stopping on Epoch 42.\n",
      "| \u001b[0m61       \u001b[0m | \u001b[0m-12.95   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m346.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 16.\n",
      "| \u001b[0m62       \u001b[0m | \u001b[0m-67.17   \u001b[0m | \u001b[0m88.12    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m851.2    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 62.\n",
      "| \u001b[0m63       \u001b[0m | \u001b[0m-12.23   \u001b[0m | \u001b[0m92.22    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m791.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 354.\n",
      "| \u001b[0m64       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m88.78    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m63.16    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 216.\n",
      "| \u001b[0m65       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m70.41    \u001b[0m | \u001b[0m0.4259   \u001b[0m | \u001b[0m101.4    \u001b[0m | \u001b[0m14.26    \u001b[0m |\n",
      "Early stopping on Epoch 117.\n",
      "| \u001b[95m66       \u001b[0m | \u001b[95m-11.87   \u001b[0m | \u001b[95m32.04    \u001b[0m | \u001b[95m0.9      \u001b[0m | \u001b[95m100.7    \u001b[0m | \u001b[95m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 39.\n",
      "| \u001b[0m67       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m796.9    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 66.\n",
      "| \u001b[0m68       \u001b[0m | \u001b[0m-13.38   \u001b[0m | \u001b[0m29.15    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m142.1    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 46.\n",
      "| \u001b[0m69       \u001b[0m | \u001b[0m-12.69   \u001b[0m | \u001b[0m1.455    \u001b[0m | \u001b[0m0.4229   \u001b[0m | \u001b[0m468.5    \u001b[0m | \u001b[0m7.621    \u001b[0m |\n",
      "Early stopping on Epoch 109.\n",
      "| \u001b[0m70       \u001b[0m | \u001b[0m-11.95   \u001b[0m | \u001b[0m103.7    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m326.4    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m71       \u001b[0m | \u001b[0m-52.23   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 142.\n",
      "| \u001b[0m72       \u001b[0m | \u001b[0m-13.74   \u001b[0m | \u001b[0m103.4    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m172.5    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 58.\n",
      "| \u001b[0m73       \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m596.5    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 45.\n",
      "| \u001b[0m74       \u001b[0m | \u001b[0m-13.52   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m709.9    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 27.\n",
      "| \u001b[0m75       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m89.37    \u001b[0m | \u001b[0m0.4598   \u001b[0m | \u001b[0m1.024e+03\u001b[0m | \u001b[0m16.53    \u001b[0m |\n",
      "Early stopping on Epoch 34.\n",
      "| \u001b[0m76       \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m543.5    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 69.\n",
      "| \u001b[0m77       \u001b[0m | \u001b[0m-12.32   \u001b[0m | \u001b[0m36.52    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m564.4    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 19.\n",
      "| \u001b[0m78       \u001b[0m | \u001b[0m-54.83   \u001b[0m | \u001b[0m127.5    \u001b[0m | \u001b[0m0.1224   \u001b[0m | \u001b[0m637.5    \u001b[0m | \u001b[0m11.96    \u001b[0m |\n",
      "Early stopping on Epoch 35.\n",
      "| \u001b[0m79       \u001b[0m | \u001b[0m-13.43   \u001b[0m | \u001b[0m1.461    \u001b[0m | \u001b[0m0.6686   \u001b[0m | \u001b[0m207.6    \u001b[0m | \u001b[0m16.02    \u001b[0m |\n",
      "Early stopping on Epoch 23.\n",
      "| \u001b[0m80       \u001b[0m | \u001b[0m-12.06   \u001b[0m | \u001b[0m2.213    \u001b[0m | \u001b[0m0.801    \u001b[0m | \u001b[0m403.5    \u001b[0m | \u001b[0m2.498    \u001b[0m |\n",
      "Early stopping on Epoch 24.\n",
      "| \u001b[0m81       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m71.91    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m989.9    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 56.\n",
      "| \u001b[0m82       \u001b[0m | \u001b[0m-12.21   \u001b[0m | \u001b[0m34.63    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m335.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 61.\n",
      "| \u001b[0m83       \u001b[0m | \u001b[0m-13.4    \u001b[0m | \u001b[0m1.515    \u001b[0m | \u001b[0m0.5957   \u001b[0m | \u001b[0m13.61    \u001b[0m | \u001b[0m10.15    \u001b[0m |\n",
      "Early stopping on Epoch 23.\n",
      "| \u001b[0m84       \u001b[0m | \u001b[0m-13.97   \u001b[0m | \u001b[0m44.14    \u001b[0m | \u001b[0m0.0538   \u001b[0m | \u001b[0m820.7    \u001b[0m | \u001b[0m2.642    \u001b[0m |\n",
      "Early stopping on Epoch 70.\n",
      "| \u001b[0m85       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m128.4    \u001b[0m | \u001b[0m0.2497   \u001b[0m | \u001b[0m474.0    \u001b[0m | \u001b[0m5.619    \u001b[0m |\n",
      "Early stopping on Epoch 474.\n",
      "| \u001b[0m86       \u001b[0m | \u001b[0m-12.26   \u001b[0m | \u001b[0m68.58    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m39.56    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 79.\n",
      "| \u001b[0m87       \u001b[0m | \u001b[0m-13.34   \u001b[0m | \u001b[0m126.6    \u001b[0m | \u001b[0m0.2487   \u001b[0m | \u001b[0m524.0    \u001b[0m | \u001b[0m3.615    \u001b[0m |\n",
      "Early stopping on Epoch 103.\n",
      "| \u001b[0m88       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m128.8    \u001b[0m | \u001b[0m0.8372   \u001b[0m | \u001b[0m326.2    \u001b[0m | \u001b[0m16.55    \u001b[0m |\n",
      "Early stopping on Epoch 95.\n",
      "| \u001b[0m89       \u001b[0m | \u001b[0m-12.25   \u001b[0m | \u001b[0m65.35    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m214.2    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 152.\n",
      "| \u001b[0m90       \u001b[0m | \u001b[0m-12.28   \u001b[0m | \u001b[0m25.4     \u001b[0m | \u001b[0m0.6597   \u001b[0m | \u001b[0m43.94    \u001b[0m | \u001b[0m2.874    \u001b[0m |\n",
      "Early stopping on Epoch 35.\n",
      "| \u001b[0m91       \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m55.55    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m710.6    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 89.\n",
      "| \u001b[0m92       \u001b[0m | \u001b[0m-12.54   \u001b[0m | \u001b[0m43.78    \u001b[0m | \u001b[0m0.6941   \u001b[0m | \u001b[0m173.0    \u001b[0m | \u001b[0m2.045    \u001b[0m |\n",
      "Early stopping on Epoch 70.\n",
      "| \u001b[0m93       \u001b[0m | \u001b[0m-13.19   \u001b[0m | \u001b[0m1.29     \u001b[0m | \u001b[0m0.7099   \u001b[0m | \u001b[0m148.2    \u001b[0m | \u001b[0m5.69     \u001b[0m |\n",
      "Early stopping on Epoch 16.\n",
      "| \u001b[0m94       \u001b[0m | \u001b[0m-68.61   \u001b[0m | \u001b[0m129.0    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m979.8    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 165.\n",
      "| \u001b[0m95       \u001b[0m | \u001b[0m-13.46   \u001b[0m | \u001b[0m128.2    \u001b[0m | \u001b[0m0.1427   \u001b[0m | \u001b[0m121.0    \u001b[0m | \u001b[0m2.141    \u001b[0m |\n",
      "Early stopping on Epoch 30.\n",
      "| \u001b[0m96       \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m113.4    \u001b[0m | \u001b[0m0.03172  \u001b[0m | \u001b[0m1.023e+03\u001b[0m | \u001b[0m3.623    \u001b[0m |\n",
      "Early stopping on Epoch 31.\n",
      "| \u001b[0m97       \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m71.95    \u001b[0m | \u001b[0m0.5127   \u001b[0m | \u001b[0m766.7    \u001b[0m | \u001b[0m15.89    \u001b[0m |\n",
      "Early stopping on Epoch 57.\n",
      "| \u001b[0m98       \u001b[0m | \u001b[0m-13.3    \u001b[0m | \u001b[0m56.04    \u001b[0m | \u001b[0m0.8615   \u001b[0m | \u001b[0m376.4    \u001b[0m | \u001b[0m16.12    \u001b[0m |\n",
      "Early stopping on Epoch 50.\n",
      "| \u001b[0m99       \u001b[0m | \u001b[0m-12.97   \u001b[0m | \u001b[0m7.878    \u001b[0m | \u001b[0m0.03297  \u001b[0m | \u001b[0m80.96    \u001b[0m | \u001b[0m2.556    \u001b[0m |\n",
      "Early stopping on Epoch 63.\n",
      "| \u001b[0m100      \u001b[0m | \u001b[0m-13.3    \u001b[0m | \u001b[0m74.73    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m317.0    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 40.\n",
      "| \u001b[0m101      \u001b[0m | \u001b[0m-13.31   \u001b[0m | \u001b[0m66.94    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m798.5    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 54.\n",
      "| \u001b[0m102      \u001b[0m | \u001b[0m-13.29   \u001b[0m | \u001b[0m95.7     \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m345.9    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "Early stopping on Epoch 16.\n",
      "| \u001b[0m103      \u001b[0m | \u001b[0m-68.01   \u001b[0m | \u001b[0m95.91    \u001b[0m | \u001b[0m0.204    \u001b[0m | \u001b[0m586.5    \u001b[0m | \u001b[0m16.5     \u001b[0m |\n",
      "Early stopping on Epoch 56.\n",
      "| \u001b[0m104      \u001b[0m | \u001b[0m-12.0    \u001b[0m | \u001b[0m54.82    \u001b[0m | \u001b[0m0.5932   \u001b[0m | \u001b[0m582.8    \u001b[0m | \u001b[0m5.176    \u001b[0m |\n",
      "Early stopping on Epoch 55.\n",
      "| \u001b[0m105      \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m128.4    \u001b[0m | \u001b[0m0.6564   \u001b[0m | \u001b[0m550.8    \u001b[0m | \u001b[0m2.18     \u001b[0m |\n",
      "Early stopping on Epoch 143.\n",
      "| \u001b[0m106      \u001b[0m | \u001b[0m-13.33   \u001b[0m | \u001b[0m71.72    \u001b[0m | \u001b[0m0.5931   \u001b[0m | \u001b[0m158.6    \u001b[0m | \u001b[0m3.285    \u001b[0m |\n",
      "Early stopping on Epoch 66.\n",
      "| \u001b[0m107      \u001b[0m | \u001b[0m-16.89   \u001b[0m | \u001b[0m30.94    \u001b[0m | \u001b[0m0.1704   \u001b[0m | \u001b[0m1.024e+03\u001b[0m | \u001b[0m7.453    \u001b[0m |\n",
      "Early stopping on Epoch 124.\n",
      "| \u001b[0m108      \u001b[0m | \u001b[0m-13.32   \u001b[0m | \u001b[0m44.41    \u001b[0m | \u001b[0m0.4511   \u001b[0m | \u001b[0m202.6    \u001b[0m | \u001b[0m16.06    \u001b[0m |\n",
      "Early stopping on Epoch 88.\n",
      "| \u001b[0m109      \u001b[0m | \u001b[0m-12.57   \u001b[0m | \u001b[0m66.12    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m558.5    \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "Early stopping on Epoch 30.\n",
      "| \u001b[0m110      \u001b[0m | \u001b[0m-13.28   \u001b[0m | \u001b[0m31.94    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m456.3    \u001b[0m | \u001b[0m17.0     \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "# iterate through feature space\n",
    "optimizer.maximize(init_points=10, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T22:05:13.703234Z",
     "start_time": "2023-11-27T22:05:13.683217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32.03638856422199,\n",
       " 'dropout': 0.9,\n",
       " 'hidden_dim': 100.74795411866992,\n",
       " 'num_layers': 2.0}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at params that gave lowest validation RMSE\n",
    "best_hyperparams = optimizer.max['params']\n",
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best val_rmse = __11.87__ with a sequence length of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023 predictions\n",
    "- This model will be trained on the entire train/val data, and then will predict 2023 offensive grade from the 2022 holdout set.\n",
    "- Since we are using a sequence length of 2 seasons to predict the third, we will use 2021 & 2022 seasons (to predict 2022 target) as the test set.\n",
    "- This means that players with under 3 seasons played can't be predicted on. I will use the best model (Random Forest) from [this notebook](./models_1.ipynb) to make these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:40:04.861932Z",
     "start_time": "2023-11-28T01:40:04.812887Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the names of the 48 players that have 2023 targets\n",
    "player_names_2023 = players_2022.player.values\n",
    "\n",
    "# master_df includes 2022 rows. create a subset for these players, get players with at least 2 seasons\n",
    "players_subset = master_df[master_df['player'].isin(player_names_2023)]\n",
    "players_subset = players_subset.groupby('player').filter(lambda x: len(x) >= 2)\n",
    "\n",
    "# get last two rows for each player\n",
    "seq_test = players_subset.groupby('player').apply(lambda x: x.tail(2)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:40:08.135974Z",
     "start_time": "2023-11-28T01:40:05.316410Z"
    }
   },
   "outputs": [],
   "source": [
    "# train sequences\n",
    "X_train, y_train = create_seq(feature_subset=all_feats, seq_len=2, df=df)\n",
    "\n",
    "# test sequences\n",
    "X_test, y_test = create_seq(feature_subset=all_feats, seq_len=2, df=seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:40:08.709878Z",
     "start_time": "2023-11-28T01:40:08.692863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((752, 2, 66), (752,), (42, 2, 66), (42,))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 752 total examples to train on, 42 QBs to predict on\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:40:10.729348Z",
     "start_time": "2023-11-28T01:40:10.711331Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_loader = create_loaders(X_train, y_train, test_size=0, batch_size=32)\n",
    "test_loader = create_loaders(X_test, y_test, test_size=0, batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:48:32.736071Z",
     "start_time": "2023-11-28T01:48:32.729064Z"
    }
   },
   "outputs": [],
   "source": [
    "# best sequence model\n",
    "best_seq = RNN(input_dim=len(all_feats), hidden_dim=98, num_layers=2, dropout=0.9).to(device)\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.AdamW(best_seq.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:48:37.369063Z",
     "start_time": "2023-11-28T01:48:32.873199Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 100 epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# training mode\n",
    "best_seq.train()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):  \n",
    "    # train batches\n",
    "    for x, y in train_loader:\n",
    "        # put x and y on gpu\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        train_preds = best_seq(x)\n",
    "        # calc loss\n",
    "        train_loss = criterion(train_preds, y)\n",
    "        # backward pass\n",
    "        train_loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:48:37.385078Z",
     "start_time": "2023-11-28T01:48:37.371066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.895\n",
      "R^2: 0.317\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "best_seq.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for x, y in test_loader:\n",
    "        # put x and y on gpu\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        test_preds = best_seq(x)\n",
    "        # calc loss\n",
    "        test_loss = criterion(test_preds, y)\n",
    "\n",
    "        # performance metrics\n",
    "        rmse = np.sqrt(((test_preds - y) ** 2).sum().item() / y.shape[0])\n",
    "        r2 = r2_score(y.numpy(force=True), test_preds.numpy(force=True))\n",
    "\n",
    "print(f'RMSE: {rmse:.3f}')\n",
    "print(f'R^2: {r2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On the 42/48 QBs who have 3+ seasons played, our model predicts their 2023 offensive grade with an RMSE of 12.895."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:49:47.599269Z",
     "start_time": "2023-11-28T01:49:47.580251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 70)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 2022 players who can't be predicted on with sequence model (players with less than 2 seasons)\n",
    "players_subset = master_df[master_df['player'].isin(player_names_2023)]\n",
    "players_subset = players_subset.groupby('player').filter(lambda x: len(x) < 2)\n",
    "\n",
    "# get last row for each player\n",
    "non_seq_test = players_subset.groupby('player').apply(lambda x: x.tail(1)).reset_index(drop=True)\n",
    "non_seq_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6 QBs in 2023 with less than 3 seasons played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:50:11.353517Z",
     "start_time": "2023-11-28T01:50:09.810113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.658\n",
      "R^2: 0.245\n"
     ]
    }
   ],
   "source": [
    "# best random forest\n",
    "best_rf = RandomForestRegressor(random_state=random_state, min_samples_split=112)\n",
    "\n",
    "# features and target\n",
    "X_train = df[all_feats]\n",
    "y_train = df.target\n",
    "X_test = non_seq_test[all_feats]\n",
    "y_test = non_seq_test.target\n",
    "\n",
    "# create pieline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('rf', best_rf)\n",
    "    ])\n",
    "\n",
    "# train on entire dataset\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "preds = pipeline.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "print(f'RMSE: {rmse:.3f}')\n",
    "print(f'R^2: {r2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T01:57:16.248421Z",
     "start_time": "2023-11-28T01:57:16.234409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.747\n",
      "R^2: 0.338\n"
     ]
    }
   ],
   "source": [
    "# combine preds from the two models\n",
    "y_pred = np.concatenate([test_preds.squeeze(-1).cpu().numpy(), preds])\n",
    "\n",
    "# get true values\n",
    "y_true = np.concatenate([y.squeeze(-1).cpu().numpy(), y_test])\n",
    "\n",
    "# look at overall performance\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f'RMSE: {rmse:.3f}')\n",
    "print(f'R^2: {r2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using an RNN with a sequence length of 2 (paired with best Random Forest), we achieve a RMSE of __12.75__.\n",
    "- This is worse performance than just the Random Forest from [models_1](./models_1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T02:36:52.739556Z",
     "start_time": "2023-11-28T02:36:52.713532Z"
    }
   },
   "outputs": [],
   "source": [
    "# player names\n",
    "player_names = seq_test.player.unique().tolist() + non_seq_test.player.unique().tolist()\n",
    "\n",
    "# teams\n",
    "team_names = []\n",
    "for _, group in seq_test.groupby('player'):\n",
    "    team_names.append(group.iloc[-1].team_name)\n",
    "\n",
    "team_names.extend(non_seq_test.team_name.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
